import sys
import subprocess
import datetime
import os
import hashlib
import re
import time
import psycopg2
from pgvector.psycopg2 import register_vector
from google import genai
from openai import OpenAI
from dotenv import load_dotenv

# --- INITIALISATION ---
load_dotenv()

# Configuration DB
DB_CONFIG = {
    "host": os.getenv("DB_HOST"),
    "port": os.getenv("DB_PORT"),
    "dbname": os.getenv("DB_NAME"),
    "user": os.getenv("DB_USER"),
    "password": os.getenv("DB_PASSWORD")
}

# Configuration Chemins
LOCAL_BIN = os.environ.get('LOCAL_BIN', os.path.expanduser(r'~\.local\bin'))
ASK_SCRIPT = os.path.join(LOCAL_BIN, 'ask.py')
PYTHON_BIN = os.environ.get('PYTHON_BIN', 'python')


# --- FONCTIONS DE SERVICE ---

def index_interaction(full_text):
    """Calcule le hash, l'embedding et ins√®re dans Postgres."""
    try:
        api_key = os.environ.get("GEMINI_API_KEY")
        if not api_key: return

        client = genai.Client(api_key=api_key)
        content_hash = hashlib.md5(full_text.encode('utf-8')).hexdigest()

        with psycopg2.connect(**DB_CONFIG) as conn:
            register_vector(conn)
            with conn.cursor() as cur:
                # V√©rification unicit√©
                cur.execute("SELECT id FROM chat_history WHERE content_hash = %s", (content_hash,))
                if cur.fetchone(): return

                # G√©n√©ration Embedding
                res = client.models.embed_content(
                    model="models/gemini-embedding-001",
                    contents=full_text,
                    config={'output_dimensionality': 768}
                )

                cur.execute(
                    "INSERT INTO chat_history (content, content_hash, embedding) VALUES (%s, %s, %s)",
                    (full_text, content_hash, res.embeddings[0].values)
                )
        print("‚úÖ M√©moire vectorielle synchronis√©e.")
    except Exception as e:
        print(f"‚ö†Ô∏è Note: √âchec de l'indexation vectorielle ({str(e)[:100]})", file=sys.stderr)


def update_global_summary(user_query, ai_response):
    """Consolide la m√©moire normative YAML avec basculement intelligent."""
    # Petite pause pour √©viter le Rate Limit (429) juste apr√®s la r√©ponse principale
    time.sleep(1)

    client = OpenAI(
        base_url="https://openrouter.ai/api/v1",
        api_key=os.getenv("OPENROUTER_API_KEY")
    )

    # Pile de mod√®les pour la consolidation
    archive_models = [
        "google/gemini-2.0-flash-001",
        "google/gemini-2.0-flash-lite-001",
        "qwen/qwen-2.5-72b-instruct:free",
        "openrouter/auto"
    ]

    summary_file = 'resume_contexte.yaml'

    if os.path.exists(summary_file):
        with open(summary_file, 'r', encoding='utf-8') as f:
            old_summary = f.read()
    else:
        old_summary = "summary: {objective: 'Initialisation', decisions: {confirmed: [], rejected: []}}"

    prompt_consolidation = f"""
Tu dois consolider la m√©moire normative utilis√©e pour la conversation.

OBJECTIF
- Produire un r√©sum√© coh√©rent et stable
- R√©duire le bruit et les informations redondantes
- Respecter les d√©cisions et contraintes √©tablies
- Pr√©parer la m√©moire pour les prochaines interactions

R√àGLES STRICTES
- Tu peux r√©√©crire la m√©moire compl√®te, mais uniquement pour la **clart√© et la coh√©rence**
- Ne supprime jamais une d√©cision confirm√©e ou rejet√©e sans raison explicite
- Les hypoth√®ses non valid√©es doivent rester dans open_questions
- Les contraintes doivent √™tre conserv√©es telles quelles
- Ne jamais inclure de contexte vectoriel ou de texte libre
- Limiter chaque item √† une phrase courte et claire
- Le r√©sum√© final doit √™tre concis (‚â§ 50 lignes si possible)

FORMAT DE SORTIE
- YAML uniquement
- Racine : summary
- Champs autoris√©s :
  - objective
  - constraints
  - decisions:
      confirmed
      rejected
  - open_questions
  - next_actions
- Aucun texte hors YAML

M√âMOIRE ACTUELLE :
{old_summary}

DERNI√àRE INTERACTION :
Utilisateur : {user_query}
IA : {ai_response[:2000]}
"""

    for model in archive_models:
        try:
            response = client.chat.completions.create(
                model=model,
                messages=[{"role": "system", "content": "Tu es un archiviste YAML."},
                          {"role": "user", "content": prompt_consolidation}],
                temperature=0.1
            )
            raw = response.choices[0].message.content
            clean_yaml = re.sub(r'```yaml|```', '', raw).strip()

            with open(summary_file, 'w', encoding='utf-8') as f:
                f.write(clean_yaml)
            print("üìä M√©moire normative (YAML) consolid√©e.")
            return
        except Exception as e:
            # Plus de transparence sur l'√©chec de consolidation
            err_msg = str(e)
            print(f"‚ö†Ô∏è √âchec consolidation avec {model} : {err_msg[:60]}...", file=sys.stderr)
            continue


# --- LOGIQUE PRINCIPALE ---

def run():
    # 1. Collecte des entr√©es (Arguments + Pipe)
    user_question = " ".join(sys.argv[1:])
    context_data = sys.stdin.read() if not sys.stdin.isatty() else ""

    if not user_question and not context_data:
        print("‚ùå Erreur : Aucun contenu fourni.")
        return

    # 2. Ex√©cution de ask.py
    try:
        result = subprocess.run(
            [PYTHON_BIN, ASK_SCRIPT, user_question],
            input=context_data,
            stdout=subprocess.PIPE,
            stderr=None,  # Stream direct du spinner et du debug de ask.py
            text=True,
            encoding='utf-8'
        )

        if result.returncode != 0:
            print(f"\n[ABORT] L'IA a rencontr√© une erreur fatale.", file=sys.stderr)
            return

        ai_response = result.stdout.strip()
        if not ai_response:
            print("‚ö†Ô∏è R√©ponse vide re√ßue de l'IA.")
            return

        # 3. √âcriture des fichiers de sortie
        timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        header = f"\n{'=' * 50}\nDATE   : {timestamp}\nPROMPT : {user_question}\n{'-' * 50}\n"
        full_entry = f"{header}{ai_response}\n"

        try:
            with open('dernier_plan.md', 'w', encoding='utf-8') as p:
                p.write(ai_response)

            with open('historique_global.md', 'a', encoding='utf-8') as h:
                h.write(full_entry)
        except OSError as e:
            print(f"‚ùå Erreur disque : {e}", file=sys.stderr)
            return

        # 4. Affichage final et t√¢ches de fond
        print(ai_response)

        # Lancement des indexations et r√©sum√©s
        index_interaction(full_entry)
        update_global_summary(user_question, ai_response)

    except Exception as e:
        print(f"‚ùå Erreur syst√®me : {e}", file=sys.stderr)


if __name__ == "__main__":
    run()
