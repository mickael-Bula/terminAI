import sys
import subprocess
import datetime
import os
import hashlib
import re
import openai

import psycopg2
from pgvector.psycopg2 import register_vector
from psycopg2.extensions import cursor
from google import genai
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()

# Configuration de la base de donn√©es
DB_CONFIG = {
    "host": os.getenv("DB_HOST"),
    "port": os.getenv("DB_PORT"),
    "dbname": os.getenv("DB_NAME"),
    "user": os.getenv("DB_USER"),
    "password": os.getenv("DB_PASSWORD")
}


def index_interaction(full_text):
    """Calcule le hash, l'embedding et ins√®re dans Postgres."""
    try:
        api_key = os.environ.get("GEMINI_API_KEY")

        if not api_key:
            return  # Pas d'API key, on ignore l'indexation

        client = genai.Client(api_key=api_key)

        # 1. Calcul de l'empreinte unique (Hash)
        content_hash = hashlib.md5(full_text.encode('utf-8')).hexdigest()

        # 2. Connexion Postgres
        conn = psycopg2.connect(**DB_CONFIG)
        register_vector(conn)
        cur: cursor = conn.cursor()

        # 3. On s'assure que le contenu n'est pas d√©j√† index√©.
        cur.execute("SELECT id FROM chat_history WHERE content_hash = %s", (content_hash,))
        if cur.fetchone():
            cur.close()
            conn.close()
            return

        # 4. G√©n√©ration de l'Embedding
        res = client.models.embed_content(
            model="models/gemini-embedding-001",
            contents=full_text,
            config={'output_dimensionality': 768}
        )
        embedding = res.embeddings[0].values

        # 5. Insertion
        cur.execute(
            "INSERT INTO chat_history (content, content_hash, embedding) VALUES (%s, %s, %s)",
            (full_text, content_hash, embedding)
        )
        conn.commit()
        cur.close()
        conn.close()
        print("\n‚úÖ M√©moire vectorielle synchronis√©e.")
    except Exception as e:
        # On affiche juste un avertissement pour ne pas bloquer le flux principal
        print(f"\n‚ö†Ô∏è Note: √âchec de l'indexation vectorielle ({e})")


def update_global_summary(user_query_only, ai_response_only):
    client = OpenAI(
        base_url="https://openrouter.ai/api/v1",
        api_key=os.getenv("OPENROUTER_API_KEY")
    )

    # Pile de mod√®les pour l'ARCHIVAGE (Priorit√© au Gratuit)
    archive_models = [
        "mistralai/mistral-saba",
        "google/gemini-2.5-flash-lite-preview-09-2025",
        "qwen/qwen-2.5-72b-instruct:free",
        "openrouter/auto"
    ]

    summary_file = 'resume_contexte.yaml'

    # On charge l'ancienne m√©moire
    if os.path.exists(summary_file):
        with open(summary_file, 'r', encoding='utf-8') as f:
            old_summary = f.read()
    else:
        old_summary = "summary: {objective: 'Initialisation', decisions: {confirmed: [], rejected: []}}"

        # Au besoin, on tronque la r√©ponse IA pour √©conomiser les tokens et √©viter le 429
        ai_response = ai_response_only[:4000] + "\n[...TRONQU√â...]"
        ai_response_only = ai_response if len(ai_response_only) > 4000 else ai_response_only

    prompt_consolidation = f"""
Tu dois consolider la m√©moire normative utilis√©e pour la conversation.

OBJECTIF
- Produire un r√©sum√© coh√©rent et stable
- R√©duire le bruit et les informations redondantes
- Respecter les d√©cisions et contraintes √©tablies
- Pr√©parer la m√©moire pour les prochaines interactions

R√àGLES STRICTES
- Tu peux r√©√©crire la m√©moire compl√®te, mais uniquement pour la **clart√© et la coh√©rence**
- Ne supprime jamais une d√©cision confirm√©e ou rejet√©e sans raison explicite
- Les hypoth√®ses non valid√©es doivent rester dans open_questions
- Les contraintes doivent √™tre conserv√©es telles quelles
- Ne jamais inclure de contexte vectoriel ou de texte libre
- Limiter chaque item √† une phrase courte et claire
- Le r√©sum√© final doit √™tre concis (‚â§ 50 lignes si possible)

FORMAT DE SORTIE
- YAML uniquement
- Racine : summary
- Champs autoris√©s :
  - objective
  - constraints
  - decisions:
      confirmed
      rejected
  - open_questions
  - next_actions
- Aucun texte hors YAML

M√âMOIRE ACTUELLE
{old_summary}

DERNI√àRE INTERACTION
Utilisateur : {user_query_only}
IA : {ai_response_only}

G√âN√àRE MAINTENANT LE R√âSUM√â CONSOLID√â EN YAML.
    """

    try:
        for model in archive_models:
            try:
                # Envoi du prompt de consolidation YAML
                response = client.chat.completions.create(
                    model=model,
                    messages=[{"role": "system", "content": "Tu es un archiviste YAML."},
                              {"role": "user", "content": prompt_consolidation}],
                    temperature=0.1  # On baisse la temp√©rature pour plus de rigueur
                )

                # R√©cup√®re le contenu brut depuis la structure d'OpenAI
                raw_content = response.choices[0].message.content

                # Nettoyage si le mod√®le met des balises Markdown
                clean_yaml = raw_content.replace('```yaml', '').replace('```', '').strip()

                with open(summary_file, 'w', encoding='utf-8') as f:
                    f.write(clean_yaml)
                print("üìä M√©moire normative (YAML) consolid√©e.")

                return
            except (openai.RateLimitError, openai.APIConnectionError,
                    openai.APITimeoutError, openai.APIError) as e:
                # Ici, on ne capture que les erreurs li√©es √† l'API pour tenter le mod√®le suivant
                print(f"‚ö†Ô∏è √âchec API avec {model} ({type(e).__name__}), tentative avec le suivant...")
                continue
            except OSError as e:
                # Erreur d'√©criture de fichier (ex : permissions), inutile de changer de mod√®le IA
                print(f"‚ùå Erreur disque : {e}")
                break

    except Exception as e:
        error_msg = str(e)
        if "429" in error_msg:
            print("\n‚ùå QUOTA √âPUIS√â POUR AUJOURD'HUI.")
            # Extraction du temps d'attente sugg√©r√© par Google
            wait_match = re.search(r"retry in ([\d\.]+)s", error_msg)
            if wait_match:
                print(f"üí° Google sugg√®re d'attendre {wait_match.group(1)} secondes.")
            print("üëâ Conseil : Change de cl√© API ou attends demain pour la consolidation.")
        else:
            print(f"‚ùå Erreur API : {e}")


def run():
    # 1. V√©rification et cr√©ation du dossier de scripts si n√©cessaire
    # On r√©cup√®re le chemin depuis l'environnement ou on utilise celui par d√©faut
    local_bin = os.environ.get('LOCAL_BIN', r'C:\Users\bulam\.local\bin')
    if not os.path.exists(local_bin):
        try:
            os.makedirs(local_bin, exist_ok=True)
        except Exception as e:
            print(f"Erreur lors de la cr√©ation du dossier {local_bin} : {e}")

    # 2. R√©cup√©rer le prompt (La question utilisateur)
    user_question = " ".join(sys.argv[1:])  # On distingue la question (argv) du contexte lourd (stdin).

    context_data = ""
    if not sys.stdin.isatty():
        context_data = sys.stdin.read()

    if not user_question and not context_data:
        print("Erreur : Aucun contenu fourni.")
        return

    # Configuration des chemins
    ask_script = os.environ.get('ASK_SCRIPT', os.path.join(local_bin, 'ask.py'))
    python_bin = os.environ.get('PYTHON_BIN', 'python')
    hist_file = 'historique_global.md'
    plan_file = 'dernier_plan.md'

    # 3. Pr√©parer l'en-t√™te de l'historique
    timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    divider = "=" * 50
    header = f"\n{divider}\nDATE   : {timestamp}\nPROMPT : {user_question}\n{'-' * 50}\n"

    # 4. Ex√©cuter ask.py et capturer la sortie
    # stdin=sys.stdin permet de transmettre le flux (ex : cat fichier | glog)
    try:
        result = subprocess.run(
            [python_bin, ask_script, user_question],
            input=context_data,  # On transmet le flux ici
            capture_output=True,
            text=True,
            encoding='utf-8'
        )

        if result.returncode != 0:
            # Si ask.py a fait un sys.exit(1), on s'arr√™te ici et on affiche l'erreur envoy√©e sur stderr.
            print(f"\n[ABORT] L'IA n'a pas pu r√©pondre :\n{result.stderr}", file=sys.stderr)
            return

        # 5. Pr√©parer le bloc complet EN M√âMOIRE d'abord (Write Once Logic)
        ai_response = result.stdout

        # On ne cr√©e la cha√Æne finale QUE si on a bien re√ßu une r√©ponse
        full_entry = f"{header}{ai_response}\n"

        # √âcriture atomique : On ouvre, on √©crit tout le bloc, on ferme imm√©diatement.
        try:
            # Mise √† jour du dernier plan (√©crase le pr√©c√©dent)
            with open(plan_file, 'w', encoding='utf-8') as p:
                p.write(ai_response)

            # Ajout √† l'historique global (ajoute √† la fin)
            # En √©crivant 'full_entry' d'un coup, on √©vite d'avoir un header sans r√©ponse
            with open(hist_file, 'a', encoding='utf-8') as h:
                h.write(full_entry)

        except OSError as e:
            print(f"‚ùå Erreur critique lors de l'√©criture des fichiers : {e}")
            return  # On arr√™te tout si le disque est plein ou prot√©g√©

        # 6. Afficher le r√©sultat dans le terminal
        print(ai_response)

        # --- AUTO-INDEXATION VECTORIELLE ---
        index_interaction(full_entry)

        # --- GENERATION DU RESUME ---
        update_global_summary(user_question, ai_response)

    except Exception as e:
        print(f"Une erreur syst√®me est survenue : {e}")


if __name__ == "__main__":
    run()
